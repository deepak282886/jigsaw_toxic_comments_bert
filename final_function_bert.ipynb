{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_function_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Data and libraries"
      ],
      "metadata": {
        "id": "FoWjseyUd-Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYBbsqSaZ3O1",
        "outputId": "0db0b6cb-ed20-439a-8f81-35f4fa01f864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (13.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.43.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.13.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.21.5)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.24.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, Input, Dropout, LSTM\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from keras.models import load_model\n",
        "import pickle"
      ],
      "metadata": {
        "id": "LLriqLogZ7AV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa059144-43bc-47c1-f586-6f5179cca5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWMxjkALgMCA",
        "outputId": "1c2a9061-4a0d-496f-8b32-8341e8cb0343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWEz1ir-gOfB",
        "outputId": "437aa7ea-8732-4592-d524-275ad33d3ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/kaggle_toxic\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/kaggle_toxic/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loading_things():\n",
        "    global bert_preprocess, bert\n",
        "    bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "    bert = load_model('bert.hdf5', custom_objects={'KerasLayer': bert_preprocess})"
      ],
      "metadata": {
        "id": "lCMuaX3EtPA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loading_things()"
      ],
      "metadata": {
        "id": "LaM0EOzHtsHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.read_csv('all_data.csv')"
      ],
      "metadata": {
        "id": "hM534bpaOSEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q1xav2xhhLO"
      },
      "outputs": [],
      "source": [
        "toxic = []\n",
        "#making comments which have probability more than 0.5 as toxic and marking them as 1 while non-toxic as 0\n",
        "for i in all_data['toxicity']:\n",
        "     if i > 0.5:\n",
        "        toxic.append(1)\n",
        "     else:\n",
        "        toxic.append(0)\n",
        "\n",
        "all_data['toxic_binary'] = toxic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0qJm6Otka3m"
      },
      "outputs": [],
      "source": [
        "all_data['sub_toxic'] = all_data[['severe_toxicity','obscene','sexual_explicit', 'identity_attack','insult', 'threat']].idxmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzxDx9KAh120"
      },
      "outputs": [],
      "source": [
        "sub_toxic = []\n",
        "for j in range(len(all_data)):\n",
        "   if all_data['toxic_binary'][j] == 1:\n",
        "      if all_data['sub_toxic'][j] == 'severe_toxicity':\n",
        "          sub_toxic.append(6)  \n",
        "      if all_data['sub_toxic'][j] == 'obscene':  \n",
        "          sub_toxic.append(5)\n",
        "      if all_data['sub_toxic'][j] == 'sexual_explicit': \n",
        "          sub_toxic.append(4)\n",
        "      if all_data['sub_toxic'][j] == 'identity_attack':     \n",
        "          sub_toxic.append(3)\n",
        "      if all_data['sub_toxic'][j] == 'insult': \n",
        "          sub_toxic.append(2)\n",
        "      if all_data['sub_toxic'][j] == 'threat':   \n",
        "          sub_toxic.append(1)\n",
        "   if all_data['toxic_binary'][j] == 0:\n",
        "       sub_toxic.append(0)\n",
        "\n",
        "all_data['sub_toxic'] = sub_toxic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper functions"
      ],
      "metadata": {
        "id": "yC5u7EvZeI6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm4SOAWLWePG"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "  text_token = word_tokenize(text)\n",
        "  filtered_text = ' '.join([w.lower() for w in text_token if len(w) > 2])\n",
        "  filtered_text = filtered_text.replace(r\"[^a-zA-Z]+\", '')\n",
        "  text_only = re.sub(r'\\b\\d+\\b', '', filtered_text)\n",
        "  clean_text = text_only.replace(',', '').replace('.', '').replace(':', '')\n",
        "  return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def printing_function(text, category):\n",
        "    result = {}\n",
        "    for i in range(len(category)):\n",
        "        if category[i] == 0:\n",
        "           result['Sentence-'+str(i)] = 'Classification : non-toxic'\n",
        "        if category[i] == 1:\n",
        "           result['Sentence-'+str(i)] = 'Classification : threat'\n",
        "        if category[i] == 2:\n",
        "           result['Sentence-'+str(i)] = 'Classification : insult'\n",
        "        if category[i] == 3:\n",
        "           result['Sentence-'+str(i)] = 'Classification : identity attack'\n",
        "        if category[i] == 4:\n",
        "           result['Sentence-'+str(i)] = 'Classification : sexual explicit'\n",
        "        if category[i] == 5:\n",
        "           result['Sentence-'+str(i)] = 'Classification : obscene'\n",
        "        if category[i] == 6:\n",
        "           result['Sentence-'+str(i)] = 'Classification : severe toxicity'\n",
        "    return result"
      ],
      "metadata": {
        "id": "Z_eTKvn_1nZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict function"
      ],
      "metadata": {
        "id": "ZodjpQjreQNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = all_data['comment_text'][19]"
      ],
      "metadata": {
        "id": "9XeloBKhlrYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_function(text):\n",
        "    if type(text) == str:\n",
        "       clean_text = [clean(str(text))]\n",
        "    else:\n",
        "       clean_text = [clean(str(x)) for x in text]\n",
        "    prediction = bert.predict(clean_text)\n",
        "    category = np.argmax(prediction,axis=1)\n",
        "    p = printing_function(text, category)\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "Sx5LW4ZAvv37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_function(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky08WqoGbWMj",
        "outputId": "3e46d3ba-e674-42de-ced2-39f8232529f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sentence-0': 'Classification : obscene'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_function_with_metric(text, true_class):\n",
        "      if type(text) == str:\n",
        "         clean_text = [clean(str(text))]\n",
        "      else:\n",
        "         clean_text = [clean(str(x)) for x in text]\n",
        "      prediction = bert.predict(clean_text)\n",
        "      category = np.argmax(prediction,axis=1)\n",
        "      p = printing_function(text, category)\n",
        "      print(p)\n",
        "      if len(category) > 1:\n",
        "         print('F1 score: ', f1_score(true_class, category, average=None))\n",
        "         metric = {}\n",
        "         for j in range(len(true_class)):\n",
        "            if category[j] == true_class[j]:\n",
        "               metric['Sentence-'+str(j)] = 'Correct prediction'\n",
        "            \n",
        "            if category[j] != true_class[j]:\n",
        "               metric['Sentence-'+str(j)] = 'Wrong prediction'\n",
        "         print(metric)\n",
        "      else:\n",
        "         if category == true_class:\n",
        "            print('Correct prediction')\n",
        "         if category != true_class:\n",
        "            print('Wrong prediction')"
      ],
      "metadata": {
        "id": "SJyXKC_8FxhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_function_with_metric(z, all_data['sub_toxic'][19])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uou2oFQRx6aU",
        "outputId": "d4e10f81-1a95-4106-9325-637347a099c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sentence-0': 'Classification : obscene'}\n",
            "Correct prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8sA5qh4bnWo1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}